<!DOCTYPE HTML>
<html>
<head>
  <meta name="baidu-site-verification" content="mFmscoluqW" />
  <meta charset="utf-8">
  
  <title>Spark（一）：介绍、初体验 | 伊布</title>
  <meta name="author" content="hubt@dtdream.com">
  
  <meta name="description" content="1 介绍Spark是一个快速、通用的集群计算系统，提供JAVA/Scala/Python API，以及一系列的高级工具：Spark SQL/MLib/GrapyX/Spark Streaming.Spark的编程语言是scala，同样采用scala的还有kafka。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Spark（一）：介绍、初体验"/>
  <meta property="og:site_name" content="伊布"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/github.ico" rel="icon" type="image/x-ico">
  <link rel="alternate" href="/atom.xml" title="伊布" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?28bfa356a7c60e170822a01142cf208e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
var option = {
  engineKey: 'f3e1951e888b8a117845'
};
(function(w,d,t,u,n,s,e){
  s = d.createElement(t);
  s.src = u;
  s.async = 1;
  w[n] = function(r){
    w[n].opts = r;
  };
  e = d.getElementsByTagName(t)[0];
  e.parentNode.insertBefore(s, e);
})(window,document,'script','//tinysou-cdn.b0.upaiyun.com/ts.js','_ts');
_ts(option);
</script>

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">伊布</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/About">About</a></li>
    
	<li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-05-13T03:18:55.000Z"><a href="/2015/05/13/spark-intro/">2015-05-13</a></time>
      
      
  
    <h1 class="title">Spark（一）：介绍、初体验</h1>
  

    </header>
    <div class="entry">
      
        <h3 id="1_介绍">1 介绍</h3><p>Spark是一个快速、通用的集群计算系统，提供JAVA/Scala/Python API，以及一系列的高级工具：Spark SQL/MLib/GrapyX/Spark Streaming.<br>Spark的编程语言是scala，同样采用scala的还有kafka。<br><a id="more"></a></p>
<h3 id="2_安装">2 安装</h3><p>我的环境使用的是CDH版本，安装时选上了Spark，手动安装请参考官网。<br>如果将Spark安装到hadoop上，需要注意其版本依赖关系。如果你的Spark底层使用了HDFS做存储，而Spark的版本与默认的hadoop一定要不同的话，需要自行编译，改脚本没用。</p>
<h3 id="3_核心：RDD">3 核心：RDD</h3><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)模型的出现主要为了解决下面两个场景：</p>
<ul>
<li>迭代计算</li>
<li>交互式数据挖掘：同一数据子集进行Ad-hoc计算</li>
</ul>
<p>RDD解决了中间结果重用的问题，不再像MR模型必须写入HDFS。RDD具有下面两个特点：</p>
<ul>
<li>分布式：分布在多个节点上，可以被并行处理；存储在HDFS或者RDD数据集，也可以缓存在内存中，从而被多个并行任务重用。</li>
<li>容错：某个节点挂掉后，丢失的RDD可以重构。</li>
</ul>
<p>RDD支持两种操作：</p>
<ul>
<li>转换。从现有RDD生成新的RDD，例如<code>map(func)</code>、<code>filter(func)</code>、<code>join()</code>等。</li>
<li>动作。将操作结果返回驱动程序或者写入存储，例如<code>reduce(func)</code>、<code>count()</code>、<code>saveAsTextFile()</code></li>
</ul>
<p>RDD还支持缓存，主要用在迭代计算中，转换时不用<em>再次计算</em>，用户可以用persist/cache等方法使中间结果的RDD数据集缓存在内存或磁盘中。<br>有关RDD的详细研究，可以参考CSDN的<a href="http://blog.csdn.net/wwwxxdddx/article/details/45647761" target="_blank" rel="external">这篇文章</a>。</p>
<h3 id="4_基本架构">4 基本架构</h3><p>Spark会将一个应用程序转为一组任务，分布在多个节点并行计算，并提供一个共享变量在这些并行任务间、任务与驱动程序间共享。<br>每个应用程序一套运行时环境，其生命周期如下：</p>
<ul>
<li>准备运行时环境：资源管理。目前spark可以使用mesos（粗细两种粒度）/yarn（仅粗粒度）来作为其资源管理器。两种方式都需要BlockManager来管理RDD缓存。</li>
<li>将任务转换为DAG图。RDD父子数据集间存在宽依赖、窄依赖。连续多个窄依赖可以归并到一个阶段并行。</li>
<li>根据调度依赖关系执行DAG图。优化：数据本地性、推测执行。</li>
<li>销毁运行时环境</li>
</ul>
<h3 id="5_Quick_start">5 Quick start</h3><p>下面基本是参照<a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">spark官网示例</a>来的。</p>
<h4 id="5-1_spark_shell">5.1 spark shell</h4><p>spark shell可以交互式的运行spark程序，可以查看中间运行结果，是个学习框架的好方法。<br><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$ spark-shell</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _<span class="command">\ </span><span class="command">\/</span> _ <span class="command">\/</span> _ `/ __/  '_/</span><br><span class="line">   /___/ .__/<span class="command">\_</span>,_/_/ /_/<span class="command">\_</span><span class="command">\ </span>  version 1.3.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_15)</span><br><span class="line">...</span><br><span class="line">scala&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>我们可以在shell中进行一些scala的操作。shell启动时会自动提供一个SparkContext对象，我们可以直接用这个对象来加载文件为RDD。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(<span class="string">"pi.py"</span>)		<span class="comment">#textFile为一个RDD</span></span><br><span class="line">textFile: org.apache.spark.rdd.RDD[String] = pi.py MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">scala&gt; textFile.count()		<span class="comment">#RDD的action</span></span><br><span class="line">res4: Long = <span class="number">41</span></span><br><span class="line">scala&gt; textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>)).count()</span><br><span class="line">res7: Long = <span class="number">2</span>				<span class="comment">#RDD的transformation+action，先生成</span></span><br><span class="line"><span class="number">15</span>/<span class="number">05</span>/<span class="number">12</span> <span class="number">19</span>:<span class="number">00</span>:<span class="number">25</span> INFO DAGScheduler: Job <span class="number">2</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.184386</span> s</span><br></pre></td></tr></table></figure></p>
<p>spark也提供了一个python语言的shell，运行<code>pyspark</code>可以进去，使用起来跟scala类似，具体可以参见官网。</p>
<p>RDD提供了跨集群、内存级的<em>cache</em>功能，对于一些频繁访问的数据集生成缓存，可以提高效率。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>))</span><br><span class="line">scala&gt; linesWithSpark.collect()</span><br><span class="line">res12: Array[String] = Array(from pyspark import SparkContext, <span class="string">"    sc = SparkContext(appName="</span>PythonPi<span class="string">")"</span>)</span><br><span class="line">scala&gt; linesWithSpark.cache()</span><br><span class="line">res13: linesWithSpark.type = MapPartitionsRDD[<span class="number">5</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line">res16: Long = <span class="number">2</span></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line">res17: Long = <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h4 id="5-2_Self-Contained_Applications">5.2 Self-Contained Applications</h4><p>实际生产环境中不可能像上面这样交互式运行，还是要自包含的应用程序。下面我们举一个源代码里python的例子，scala和java需要配合sbt和maven，具体请参照官网。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">"""</span><br><span class="line">        Usage: pi [partitions]</span><br><span class="line">    """</span></span><br><span class="line">    sc = SparkContext(appName=<span class="string">"PythonPi"</span>)			<span class="comment">#sc要自己创建</span></span><br><span class="line">    partitions = int(sys.argv[<span class="number">1</span>]) <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">    n = <span class="number">100000</span> * partitions</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(_)</span>:</span></span><br><span class="line">        x = random() * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">        y = random() * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> x ** <span class="number">2</span> + y ** <span class="number">2</span> &lt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    count = sc.parallelize(xrange(<span class="number">1</span>, n + <span class="number">1</span>), partitions).map(f).reduce(add)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Pi is roughly %f"</span> % (<span class="number">4.0</span> * count / n)</span><br><span class="line"></span><br><span class="line">    sc.stop()	<span class="comment">#停掉sc</span></span><br></pre></td></tr></table></figure></p>
<p>简单来说，spark应用程序比shell方式多了SparkContext的创建、销毁，其他基本是一致的。<br>使用spark-submit将此python脚本提交到spark执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit pi.py <span class="number">10</span></span><br><span class="line"><span class="number">15</span>/<span class="number">05</span>/<span class="number">12</span> <span class="number">19</span>:<span class="number">42</span>:<span class="number">26</span> INFO DAGScheduler: Job <span class="number">0</span> finished: reduce at /opt/cloudera/parcels/CDH-<span class="number">5.4</span>.<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">27</span>/lib/spark/examples/lib/pi.py:<span class="number">38</span>, took <span class="number">1.867190</span> s</span><br><span class="line">Pi is roughly <span class="number">3.142040</span></span><br></pre></td></tr></table></figure></p>
<p>如果python程序依赖其他的文件或第三方的lib库，可以将其打包为zip文件，用—py-files指定就可以了。python系统库不需要。</p>
<p>Spark提供了一个history web server，可以看到我们运行了的那些程序以及他们的详细信息。<br><img src="http://7xir15.com1.z0.glb.clouddn.com/spark_jobs.PNG" alt="spark jobs"></p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/spark/">spark</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
	<div class="ds-thread"></div>
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"silenceshell"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
<form>
    <input type="text" id="ts-search-input"  name="word" maxlength="20"  class="search-form-input" placeholder="Search">
</form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">標籤</h3>
  <ul class="entry">
  
    <li><a href="/tags/PALO/">PALO</a><small>1</small></li>
  
    <li><a href="/tags/ambari/">ambari</a><small>1</small></li>
  
    <li><a href="/tags/baidu/">baidu</a><small>1</small></li>
  
    <li><a href="/tags/fullnat/">fullnat</a><small>2</small></li>
  
    <li><a href="/tags/hadoop/">hadoop</a><small>9</small></li>
  
    <li><a href="/tags/hdfs/">hdfs</a><small>1</small></li>
  
    <li><a href="/tags/hibernate/">hibernate</a><small>1</small></li>
  
    <li><a href="/tags/hive/">hive</a><small>1</small></li>
  
    <li><a href="/tags/keepalived/">keepalived</a><small>3</small></li>
  
    <li><a href="/tags/linux/">linux</a><small>1</small></li>
  
    <li><a href="/tags/lvs/">lvs</a><small>1</small></li>
  
    <li><a href="/tags/mesos/">mesos</a><small>1</small></li>
  
    <li><a href="/tags/mysql/">mysql</a><small>2</small></li>
  
    <li><a href="/tags/nginx/">nginx</a><small>1</small></li>
  
    <li><a href="/tags/olap/">olap</a><small>1</small></li>
  
    <li><a href="/tags/python/">python</a><small>2</small></li>
  
    <li><a href="/tags/spark/">spark</a><small>1</small></li>
  
    <li><a href="/tags/sqoop/">sqoop</a><small>1</small></li>
  
    <li><a href="/tags/storm/">storm</a><small>1</small></li>
  
    <li><a href="/tags/tez/">tez</a><small>1</small></li>
  
    <li><a href="/tags/yarn/">yarn</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 hubt@dtdream.com
  
</div>
<div class="clearfix"></div></footer>
  <script src="//libs.baidu.com/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>