<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[伊布]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://yoursite.com/"/>
  <updated>2015-08-08T05:55:43.418Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name><![CDATA[hubt@dtdream.com]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[百度PALO技术整理分析]]></title>
    <link href="http://yoursite.com/2015/08/08/baidu-palo/"/>
    <id>http://yoursite.com/2015/08/08/baidu-palo/</id>
    <published>2015-08-08T05:39:26.000Z</published>
    <updated>2015-08-08T05:55:43.418Z</updated>
    <content type="html"><![CDATA[<p>以下内容基本源于百度PALO（OLAP分析引擎）对外的一个<a href="http://www.chinahadoop.cn/course/95/learn#lesson/1333" target="_blank" rel="external">视频演讲</a>，讲的比较形象，下面做了简单的摘抄。</p>
<p><strong>总体架构图</strong><br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_1.png" alt=""></p>
<p>PALO对外体现为一个mysql的服务器，所有mysql相关的工具：mysql client，JDBC，基于mysql的报表工具、R语言等，都可以直接对接PALO。<br>FE即Front End，PALO使用了RAFT协议来共享元数据，所有FE都可以接收用户请求。RAFT协议类似PAXOS(代表实现zookeeper)，但要简化很多。master故障后，follower可以根据checkpoint进行LOG回放，快速恢复元数据（内存数据库）。RAFT支持较好的横向扩展，observer就是这个作用。</p>
<p><strong>数据存储</strong><br>hash partition<br>指定表按某一列做hash分区。<br>elastic range partition<br>与hash的散列分布不同，range partition是将数据连续的分区。如何解决列的value范围无法预测的问题呢？开始的时候只有一个分区列，当数据量增长到某一临界点时，将数据平均的分成两份。后面继续类似的处理。这样数据可以比较均衡的分散到不同的分区中。range partition对where友好。</p>
<p><strong>动态rollup表</strong><br>发现如果用户经常只观察该表的某几个维度，则动态生成rollup表，新表里的维度表只有常用列，由于会有一定的合并，后续查找该表的速度会比较快</p>
<p><strong>Compation</strong><br>小批量插入的数据，文件越来越多，会导致查询性能越来越差。可以设置一个规则，每隔一段时间将一定批量的数据合并在一起，查询的数据块变少，并且索引更准确，可以提高性能</p>
<p><strong>行列存</strong><br>每块数据包含256行，快内列式存储，按块压缩。数据压缩比率不高，对于分析型的数据库来说，每次查询都要遍历所有数据库，效率低。<br>稀疏索引：数据的共同特征作为索引常驻内存（每个块对应一个索引），查询时先查索引，找到数据块以后再做解压缩。</p>
<p><strong>列式存储</strong><br>参考HBASE。带来的优势：<br>1、分析型数据库通常只涉及某些列，可以避免遍历所有数据，减少CPU/IO消耗。PALO采用的是列式存储。<br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_2.png" alt=""><br>2、存储：列的数据类型一致，压缩效率高<br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_3.png" alt=""><br>3、智能索引：PALO会为每一个数据库都生成min max sum，在where sum等计算的时候，可以大幅度提高性能。<br>4、复合分区<br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_4.png" alt=""></p>
<p><strong>库内分析</strong><br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_5.png" alt=""><br>区别于数据与计算分离，可以解决网络和前端分析机器 性能的瓶颈。要求数据库有计算能力。<br>方法：数据库提供UDF/UDAF/UDTF。</p>
<p><strong>向量执行引擎</strong><br><img src="http://7xir15.com1.z0.glb.clouddn.com/palo_6.png" alt=""><br>区别于遍历所有行、过滤、计算的模型，向量执行引擎可以将待计算的该列单独拿出来计算。带来的好处：<br>行式处理变为列式处理，避免了指令和数据的cache miss；<br>编译器友好，可以循环展开+分支预测</p>
<p><strong>数据导入</strong><br>PALO依赖于hadoop，数据必须得先上HDFS，然后分布式的将各个节点上的数据块导入到PALO，导入性能较好。<br>PALO不支持直接实时插入。</p>
<p><del>小批量更新、批量原子提交</del></p>
<p><strong>其他</strong></p>
<ul>
<li>分布式管理框架，故障快速切换、恢复</li>
<li>查询引擎：share-noting MPP，可扩展好</li>
<li>大表分布式join：<br>shuffle，即先hash partition，再做join，join数据量小，适合大量数据与大量数据之间join；<br>broadcast，适合大量数据与小批量数据之间join，小批量数据直接与大批量数据的所有分片做join</li>
<li>谓词下推<br>实际就是尽早的将where条件下沉到数据库（离数据源越近越好），通过索引可以提早过滤掉数据，减少分析的数据量</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>以下内容基本源于百度PALO（OLAP分析引擎）对外的一个<a href="http://www.chinahadoop.cn/course/95/learn#lesson/1333" target="_blank" rel="external">视频演讲</a>，讲的比较]]>
    </summary>
    
      <category term="PALO" scheme="http://yoursite.com/tags/PALO/"/>
    
      <category term="baidu" scheme="http://yoursite.com/tags/baidu/"/>
    
      <category term="olap" scheme="http://yoursite.com/tags/olap/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用nginx+keepalived实现RESTful API服务器的负载均衡和高可靠性]]></title>
    <link href="http://yoursite.com/2015/07/02/nginx-keepalived/"/>
    <id>http://yoursite.com/2015/07/02/nginx-keepalived/</id>
    <published>2015-07-02T14:08:53.000Z</published>
    <updated>2015-07-02T14:43:02.000Z</updated>
    <content type="html"><![CDATA[<p>核心需求是我们有一个RESTful API的服务集群，需要能够保证不管是web服务故障还是服务器整体故障，外部访问不会间断，并且在整体运行正常的时候，需要能够负载均衡。<br>业界比较常见的几个负载均衡方案有haproxy, nginx, lvs。有关这仨的比较，可以看<a href="http://www.csdn.net/article/2014-07-24/2820837" target="_blank" rel="external">这篇文章</a>。我这里选择的方案是nginx+keepalived。nginx做反向代理，可以实现负载均衡，如果后端的web服务故障了，nginx可以实现切换；但nginx本身存在单点故障，需要通过keepalived监测实现nginx的切换。</p>
<a id="more"></a>
<p>整体结构图<br><img src="http://7xir15.com1.z0.glb.clouddn.com/nginx反向代理.png" alt=""></p>
<h2 id="1、设置nginx-repo：">1、设置nginx.repo：</h2><p>我用的操作系统是centos6.5，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/centos/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>如果是RHEL:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/rhel/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="2、安装nginx、keepalived">2、安装nginx、keepalived</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install nginx</span><br><span class="line">$ yum install keepalived</span><br></pre></td></tr></table></figure>
<p>安装完毕后两个服务都是停止的，需要start并加到系统启动服务中。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ chkconfig  nginx on</span><br><span class="line">$ chkconfig  keepalived on</span><br></pre></td></tr></table></figure></p>
<p>nginx启动后，默认会有一个http server，例如我这里访问的地址是<code>http://192.168.80.165</code>和<code>http://192.168.80.166</code>，两台服务器的地址。但实际上我不需要这俩web服务器，而是需要让nginx做反向代理，将http请求导引到我的RESTful API服务器上，配置下面会有提到。</p>
<h2 id="3、修改keepalived的配置文件">3、修改keepalived的配置文件</h2><p>配置文件的路径是<code>/etc/keepalived/keepalived.conf</code>。<br>master：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface eth2   <span class="comment">#具体的网卡</span></span><br><span class="line">    virtual_router_id <span class="number">51</span></span><br><span class="line">    priority <span class="number">101</span></span><br><span class="line">    advert_int <span class="number">1</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS</span><br><span class="line">        auth_pass <span class="number">1111</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        <span class="number">192.168</span>.<span class="number">80.111</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>slave：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface eth4   <span class="comment">#具体的网卡</span></span><br><span class="line">    virtual_router_id <span class="number">51</span></span><br><span class="line">    priority <span class="number">100</span>     <span class="comment">#比master小</span></span><br><span class="line">    advert_int <span class="number">1</span></span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_<span class="built_in">type</span> PASS</span><br><span class="line">        auth_pass <span class="number">1111</span></span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        <span class="number">192.168</span>.<span class="number">80.111</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其他的virtual server等信息都删掉，那是给lvs用的。<br>配置完毕后，重启两台服务器上的keepalived进程，在master上可以看到我们配置的虚IP（ifconfig看不到）。将master上keepalived服务stop掉，可以看到虚IP跑到slave上了；再启动master上keepalived进程，虚IP会被master抢占回来，因为master的priority更大。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ip add</span></span><br><span class="line">...</span><br><span class="line"><span class="number">2</span>: eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="number">1500</span> qdisc pfifo_fast state UP qlen <span class="number">1000</span></span><br><span class="line">    link/ether <span class="number">00</span>:<span class="number">0</span>c:<span class="number">29</span>:<span class="number">73</span>:f6:<span class="number">15</span> brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet <span class="number">192.168</span>.<span class="number">80.165</span>/<span class="number">24</span> brd <span class="number">192.168</span>.<span class="number">80.255</span> scope global eth2</span><br><span class="line">    inet <span class="number">192.168</span>.<span class="number">80.111</span>/<span class="number">32</span> scope global eth2</span><br></pre></td></tr></table></figure></p>
<h2 id="4、使用脚本检测nginx服务">4、使用脚本检测nginx服务</h2><p>上面的配置可以保证keepalived关闭（例如服务器故障）时，虚IP自动切换；但有的时候可能只是web服务故障了，我们希望的是keepalived检测服务的状态，并且能够自动切换。这种情况可以用脚本来检测nginx服务状态，根据检测结果调高或调低vrrp的优先级，达到虚IP切换的目的。<br>新建一个探测脚本：check_nginx.sh<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line">netstat -antp|grep nginx</span><br><span class="line"><span class="built_in">exit</span> $?</span><br></pre></td></tr></table></figure></p>
<p>修改keepalived.conf：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vrrp_script check_succ &#123;</span><br><span class="line">    script <span class="string">"/etc/keepalived/check_nginx.sh"</span></span><br><span class="line">    interval <span class="number">1</span></span><br><span class="line">    weight -<span class="number">5</span></span><br><span class="line">    fall <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line">vrrp_script check_fail &#123;</span><br><span class="line">    script <span class="string">"/etc/keepalived/check_nginx.sh"</span></span><br><span class="line">    interval <span class="number">1</span></span><br><span class="line">    weight <span class="number">5</span></span><br><span class="line">    rise <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">...</span><br><span class="line">    track_script &#123;</span><br><span class="line">       check_succ</span><br><span class="line">       check_fail</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>据说探测成功或失败了以后只会改一次优先级，所以不要担心不停探测优先级一直增长的问题。<br>简单说明下，上面的脚本简单的检查了nginx是不是还在监听端口，如果发现不是（例如主的nginx被stop），则priority-5，vrrp通告出去后，备发现自己的优先级更高，vrrp切换，备抢占虚IP，此时访问的nginx就是备上的了；等到主nginx重新启动后，脚本检查端口已在监听，则priority+5，vrrp切换，主会重新抢占虚IP，达到HA的目的。</p>
<h2 id="5、配置nginx">5、配置nginx</h2><p>上面配置完keepalived后，HA的功能完成了，但是用户只能访问一个服务器，对于有多个web容器的情况就无能为力了，这时候需要nginx出马。<br>nginx在我们的组网里实际是一个loadbalance的角色，将用户的请求分发给不同的server（即upstream）。由于我们后端服务器监听的是8443 ssl端口，所以步骤稍微复杂一点。</p>
<h3 id="5-1_配置nginx">5.1 配置nginx</h3><p>CENTOS6.5的nginx配置是在<code>/etc/nginx/conf.d/default</code>，我直接给出配置（基本是照抄了以升的说明）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">upstream dxt</span><br><span class="line">&#123;</span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">80.165</span>:<span class="number">8443</span>;   <span class="comment">#负载分担的两个服务器,</span></span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">80.166</span>:<span class="number">8443</span>;   <span class="comment">#也就是我这里的rest api服务，分在两台服务器上</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       <span class="number">443</span> ssl;			<span class="comment">#由于nginx和restapi服务在同一台服务器上，需要使用不同的端口</span></span><br><span class="line">    server_name  <span class="number">192.168</span>.<span class="number">80.111</span>;	<span class="comment">#虚IP</span></span><br><span class="line"></span><br><span class="line">    root html;</span><br><span class="line">    index index.html index.htm;</span><br><span class="line"></span><br><span class="line">    ssl on;							<span class="comment">#配置ssl</span></span><br><span class="line">    ssl_certificate server.crt;</span><br><span class="line">    ssl_certificate_key server.key;</span><br><span class="line">    ssl_session_timeout <span class="number">5</span>m;</span><br><span class="line"></span><br><span class="line">    ssl_protocols SSLv3 TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>;</span><br><span class="line">    ssl_ciphers <span class="string">"HIGH:!aNULL:!MD5 or HIGH:!aNULL:!MD5:!3DES"</span>;</span><br><span class="line">    ssl_prefer_server_ciphers on;</span><br><span class="line"></span><br><span class="line">    location /api/ &#123;				<span class="comment">#只处理/api这个路径</span></span><br><span class="line">        proxy_<span class="built_in">set</span>_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_<span class="built_in">set</span>_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        proxy_<span class="built_in">set</span>_header Host <span class="variable">$host</span>;</span><br><span class="line">        proxy_<span class="built_in">set</span>_header X-NginX-Proxy <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        proxy_pass https://dxt;		<span class="comment">#指定upstream</span></span><br><span class="line">        proxy_redirect off;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="5-2_配置ssl需要的server-crt、server-key">5.2 配置ssl需要的server.crt、server.key</h3><p>使用ssl还需要两个证书文件，这里也按照以升给出的方法生成。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /etc/nginx</span></span><br><span class="line"><span class="comment"># openssl genrsa -des3 -out server.key 1024</span></span><br><span class="line"><span class="comment"># openssl req -new -key server.key -out server.csr</span></span><br><span class="line"><span class="comment"># cp server.key server.key.org</span></span><br><span class="line"><span class="comment"># openssl rsa -in server.key.org -out server.key</span></span><br><span class="line"><span class="comment"># openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt</span></span><br></pre></td></tr></table></figure></p>
<p>配置后重启nginx，浏览器访问<code>https://192.168.80.111:443/api/</code>，应该可以看到restapi的信息了。</p>
<h3 id="5-3_坑">5.3 坑</h3><p>！！但是这里有个坑，如果你跟我一样也是用的centos6.5，会发现浏览器返回的是这样的：<br><img src="http://7xir15.com1.z0.glb.clouddn.com/nginx错误.png" alt="nginx错误信息"><br>说明还停留在nginx上，反向代理失败了。<br>去查nginx的日志<code>/var/log/nginx/error.log</code>，看到下面的信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">09</span>:<span class="number">52</span>:<span class="number">12</span> [error] <span class="number">15053</span><span class="comment">#0: *8 SSL_do_handshake() failed (SSL: error:100AE081:elliptic curve routines:EC_GROUP_new_by_curve_name:unknown group error:1408D010:SSL routines:SSL3_GET_KEY_EXCHANGE:EC lib) while SSL handshaking to upstream, client: 192.168.80.1, server: 192.168.80.111, request: "GET /api/ HTTP/1.1", upstream: "https://192.168.80.166:8443/api/", host: "192.168.80.111"</span></span><br></pre></td></tr></table></figure></p>
<p>查了一下，说这是centos6.5上默认openssl版本的错误，需要更新openssl的版本。可以查看<a href="http://zh.hortonworks.com/community/forums/topic/ambari-agent-registration-failure-on-rhel-6-5-due-to-openssl-2/" target="_blank" rel="external">这篇文章</a>，或者你懒的看，直接<code>yum update openssl</code>即可。<br>升级以后的版本应该是这个：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rpm -aq|grep openssl</span></span><br><span class="line">openssl-<span class="number">1.0</span>.<span class="number">1</span>e-<span class="number">30</span>.el6.<span class="number">11</span>.x86_64</span><br></pre></td></tr></table></figure></p>
<p>升级完毕后再重启一下nginx，现在访问虚IP，就能看到restapi的信息了。如果你用POSTMAN这种restapi客户端打几次请求，从rest server日志里可以看到是轮询访问不同的rest server。<br><img src="http://7xir15.com1.z0.glb.clouddn.com/虚IP.png" alt=""></p>
<hr>
<p>通过上面的keepalived和nginx的配置，我们完成了开始预设的要求：<br>1、rest server能够负载分担；<br>2、某rest server进程故障，可以由nginx剔除；<br>3、nginx故障，keepalived可以切换虚IP到正常nginx，由新的nginx继续负载分担；nginx故障恢复，切换回原来的主server；<br>4、整设备故障，vrrp超时切换虚IP到正常服务器；故障恢复，回切。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>核心需求是我们有一个RESTful API的服务集群，需要能够保证不管是web服务故障还是服务器整体故障，外部访问不会间断，并且在整体运行正常的时候，需要能够负载均衡。<br>业界比较常见的几个负载均衡方案有haproxy, nginx, lvs。有关这仨的比较，可以看<a href="http://www.csdn.net/article/2014-07-24/2820837">这篇文章</a>。我这里选择的方案是nginx+keepalived。nginx做反向代理，可以实现负载均衡，如果后端的web服务故障了，nginx可以实现切换；但nginx本身存在单点故障，需要通过keepalived监测实现nginx的切换。</p>]]>
    
    </summary>
    
      <category term="keepalived" scheme="http://yoursite.com/tags/keepalived/"/>
    
      <category term="nginx" scheme="http://yoursite.com/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hibernate之Hello World]]></title>
    <link href="http://yoursite.com/2015/06/10/hibernate-hello/"/>
    <id>http://yoursite.com/2015/06/10/hibernate-hello/</id>
    <published>2015-06-10T11:22:01.000Z</published>
    <updated>2015-06-10T11:30:11.066Z</updated>
    <content type="html"><![CDATA[<p>Hibernate是一个优秀的持久化框架，其主要功能是将内存里的瞬时状态，通过JDBC持久化到硬盘数据库上。Hibernate以面向对象的思想来解决数据库的问题，可以简化数据库的访问。<br>这篇文章通过一个简单的示例，来建立Hibernate的初步认识，较水，记录用。<br><a id="more"></a></p>
<p>示例代码基本是从这篇<a href="http://www.javaweb.cc/architecture/hibernate/132325.shtml" target="_blank" rel="external">文章</a>里抄来的。</p>
<p>首先，得有一个<del>女朋友</del>表。我在mytest数据库里创建了一个student表。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> auto_increment,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">default</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">default</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> <span class="keyword">KEY</span>  (<span class="string">`id`</span>)</span><br><span class="line">) ;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="1、创建工程，导入Hibernate库。">1、创建工程，导入Hibernate库。</h3><p>我下载的版本是4.3.10，从<a href="http://hibernate.org/orm/downloads/" target="_blank" rel="external">官方网站</a>上下载下来的是Hibernate ORM（hibernate-release-4.3.10.Final.zip）。要导入的lib文件在<code>.\hibernate-release-4.3.10.Final\lib\required\</code>，其他的先不管（我也不知道干啥用的）。<br>想不起来当时怎么把lib导入到IDEA了，现在导入没有图标了。</p>
<h3 id="2、建立包com-xxx-hibernatest">2、建立包<code>com.xxx.hibernatest</code></h3><h3 id="3、建立实体类的映射">3、建立实体类的映射</h3><p>在hibernatest包里新建Student.hbm.xml文件，编写对象关系映射文件，把对象关系映射的逻辑放在此处，这个文件包括表和字段的对象关系，当操作对象时，该文件通过java反射机制产生的方法，会把对象的方法转为关系的方法。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="doctype">&lt;!DOCTYPE hibernate-mapping PUBLIC</span><br><span class="line">        "-//Hibernate/Hibernate Mapping DTD 3.0//EN"</span><br><span class="line">        "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">hibernate-mapping</span> <span class="attribute">package</span>=<span class="value">"com.dtdream.hibernatest"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"Student"</span> <span class="attribute">table</span>=<span class="value">"student"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">id</span> <span class="attribute">name</span>=<span class="value">"id"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">generator</span> <span class="attribute">class</span>=<span class="value">"native"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"name"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"age"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">class</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="title">hibernate-mapping</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4、对象类定义。">4、对象类定义。</h3><p>在hibernatest包里新建Student.java。它对应于数据库的一个具体的表，需要定义数据库的列、列方法等，实际就是用Java来描述表。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.dtdream.hibernatest;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span>  <span class="keyword">int</span> age;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="5、main入口">5、main入口</h3><p>这里可以看到Hibernate简单的用法，代码看一看很清楚，就不多说了，名字随便起。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.dtdream.hibernatest;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.hibernate.Session;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.SessionFactory;</span><br><span class="line"><span class="keyword">import</span> org.hibernate.cfg.Configuration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Student s = <span class="keyword">new</span> Student();</span><br><span class="line">        <span class="comment">//s.setId();	这句不需要，我们创建的数据库是increasement的</span></span><br><span class="line">        s.setAge(<span class="number">23</span>);</span><br><span class="line">        s.setName(<span class="string">"dtd"</span>);</span><br><span class="line"></span><br><span class="line">        Configuration cfg = <span class="keyword">new</span> Configuration();</span><br><span class="line">        SessionFactory sf = cfg.configure().buildSessionFactory();</span><br><span class="line">        Session session = sf.openSession();</span><br><span class="line">        session.beginTransaction();</span><br><span class="line">        session.save(s);</span><br><span class="line">        session.getTransaction().commit();</span><br><span class="line">        session.close();</span><br><span class="line">        sf.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="6、Hibernate主配置">6、Hibernate主配置</h3><p>前面我们有了数据库操作主流程，有了对象关系映射文件，有了表的对象，表在前面也创建了，那么就剩下怎么访问数据库了：地址、端口号、数据库名、driver。<br>这里提供一个<code>hibernate.cfg.xml</code>的数据库配置文件。配置文件放到工程的根目录，不要放到包里。<br>Hibernate实际是封装了JDBC，需要对应数据库的driver lib，对端数据库用的是mysql，所以需要把mysql-connector-java-5.1.35-bin也加到project的lib库里。<br>配置文件就不说了，照着葫芦画瓢就行。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version='1.0' encoding='utf-8'?&gt;</span></span><br><span class="line"><span class="doctype">&lt;!DOCTYPE hibernate-configuration PUBLIC</span><br><span class="line">        "-//Hibernate/Hibernate Configuration DTD 3.0//EN"</span><br><span class="line">        "http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">hibernate-configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">session-factory</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Database connection settings --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"connection.driver_class"</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"connection.url"</span>&gt;</span>jdbc:mysql://192.168.5.29:3306/mytest<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"connection.username"</span>&gt;</span>root<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"connection.password"</span>&gt;</span><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- JDBC connection pool (use the built-in) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"connection.pool_size"</span>&gt;</span>1<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- SQL dialect --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"dialect"</span>&gt;</span>org.hibernate.dialect.MySQLDialect<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Enable Hibernate's automatic session context management --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"current_session_context_class"</span>&gt;</span>thread<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Disable the second-level cache  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"cache.provider_class"</span>&gt;</span>org.hibernate.cache.internal.NoCacheProvider<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Echo all executed SQL to stdout --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"show_sql"</span>&gt;</span>true<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Drop and re-create the database schema on startup --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span> <span class="attribute">name</span>=<span class="value">"hbm2ddl.auto"</span>&gt;</span>update<span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">mapping</span> <span class="attribute">resource</span>=<span class="value">"com/dtdream/hibernatest/Student.hbm.xml"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">session-factory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">hibernate-configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>好了，有了上面4个文件，一个简单的Hibernate示例就完成了，run main文件，我们去看mysql数据库里，可以看到添加了一条记录：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from student;</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line">| id | name   | password | age  |</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line">|  1 | dtd    | NULL     |   23 |</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line">1 rows in set (0.01 sec)</span><br></pre></td></tr></table></figure></p>
<p>再运行一次，可以看到又添加了一条记录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from student;</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line">| id | name   | password | age  |</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line">|  <span class="number">1</span> | dtd    | NULL     |   <span class="number">23</span> |</span><br><span class="line">|  <span class="number">2</span> | dtd    | NULL     |   <span class="number">23</span> |</span><br><span class="line">+----+--------+----------+------+</span><br><span class="line"><span class="number">2</span> rows <span class="keyword">in</span> <span class="built_in">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure></p>
<p>至此，示例结束，以后有机会用Hibernate做一些实际的事情再来分享更深入的东西。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hibernate是一个优秀的持久化框架，其主要功能是将内存里的瞬时状态，通过JDBC持久化到硬盘数据库上。Hibernate以面向对象的思想来解决数据库的问题，可以简化数据库的访问。<br>这篇文章通过一个简单的示例，来建立Hibernate的初步认识，较水，记录用。<br>]]>
    
    </summary>
    
      <category term="hibernate" scheme="http://yoursite.com/tags/hibernate/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[MapReduce具体问题（二）]]></title>
    <link href="http://yoursite.com/2015/06/02/mr-detail-2/"/>
    <id>http://yoursite.com/2015/06/02/mr-detail-2/</id>
    <published>2015-06-02T15:09:19.000Z</published>
    <updated>2015-06-02T15:11:47.000Z</updated>
    <content type="html"><![CDATA[<p>前一篇文章解答了Map任务数、启动人的细节，下面我们解答第二个问题：<br>HDFS的block是否粗暴但忠实的将文件按照64MB分片呢？如果是的话，怎么保证Map获取到的Splits是正确的？具体到wordcount，MR是怎么处理一个单词跨block的情况呢？</p>
<a id="more"></a>
<p>我们从Map任务的人口开始说起。前面YARN分析的时候有提到过，AppMaster会将task提交到NodeManager，在NM的container里运行具体的任务。具体到MR来说，运行的任务就是MapTask/ReduceTask。<br>来看MapTask的runNewMapper：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">runNewMapper</span><span class="params">(<span class="keyword">final</span> JobConf job,</span><br><span class="line">                    <span class="keyword">final</span> TaskSplitIndex splitIndex,</span><br><span class="line">                    <span class="keyword">final</span> TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    )</span> </span><br><span class="line">    org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt; inputFormat </span>=</span><br><span class="line">      (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt;)	<span class="comment">//定义inputFormat</span></span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);</span><br><span class="line">..</span><br><span class="line">    org.apache.hadoop.mapreduce.RecordReader&lt;INKEY,INVALUE&gt; input =</span><br><span class="line">      <span class="keyword">new</span> NewTrackingRecordReader&lt;INKEY,INVALUE&gt;</span><br><span class="line">        (split, inputFormat, reporter, taskContext);	<span class="comment">//源自inputFormat</span></span><br><span class="line">..</span><br><span class="line">    mapContext = </span><br><span class="line">      <span class="keyword">new</span> MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(job, getTaskID(), </span><br><span class="line">          input, output, 	<span class="comment">//注意input</span></span><br><span class="line">          committer, </span><br><span class="line">          reporter, split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;.Context </span><br><span class="line">        mapperContext = </span><br><span class="line">          <span class="keyword">new</span> WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;().getMapContext(</span><br><span class="line">              mapContext);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      input.initialize(split, mapperContext);	<span class="comment">//先init</span></span><br><span class="line">      mapper.run(mapperContext);				<span class="comment">//再run</span></span><br></pre></td></tr></table></figure></p>
<p>runNewMapper会先new一个mapContext，然后封装为mapperContext，并将这个context传递给mapper的run方法。显然这里只是封装上下文，并不会处理跨分片。继续来看Mapper类的run方法。<br>用户会继承Mapper类，实现自己的setup和map方法，而run方法通常直接用Mapper的。来看Map框架的run方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mapper</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">..</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (context.nextKeyValue()) &#123;		<span class="comment">//关键点</span></span><br><span class="line">        map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>简单来说就是context不断的<code>nextKeyValue</code>，得到了KV交给用户自定义的map方法。那么解决问题的关键就在nextKV了。<br>Mapper的run方法里用的是Context，具体nextKeyValue是在哪个类里定义的，还得回去MapTask里找<code>MapContextImpl</code>。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapContextImpl</span>&lt;<span class="title">KEYIN</span>,<span class="title">VALUEIN</span>,<span class="title">KEYOUT</span>,<span class="title">VALUEOUT</span>&gt; </span><br><span class="line">..</span><br><span class="line">  <span class="title">public</span> <span class="title">MapContextImpl</span>(<span class="title">Configuration</span> <span class="title">conf</span>, <span class="title">TaskAttemptID</span> <span class="title">taskid</span>,</span><br><span class="line">                        <span class="title">RecordReader</span>&lt;<span class="title">KEYIN</span>,<span class="title">VALUEIN</span>&gt; <span class="title">reader</span>,</span><br><span class="line">                        <span class="title">RecordWriter</span>&lt;<span class="title">KEYOUT</span>,<span class="title">VALUEOUT</span>&gt; <span class="title">writer</span>,</span><br><span class="line">                        <span class="title">OutputCommitter</span> <span class="title">committer</span>,</span><br><span class="line">                        <span class="title">StatusReporter</span> <span class="title">reporter</span>,</span><br><span class="line">                        <span class="title">InputSplit</span> <span class="title">split</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(conf, taskid, writer, committer, reporter);</span><br><span class="line">    <span class="keyword">this</span>.reader = reader;	<span class="comment">//reader</span></span><br><span class="line">    <span class="keyword">this</span>.split = split;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> reader.nextKeyValue();	<span class="comment">//调用reader的nextKeyValue</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>MapTask在创建上下文的时候记录reader类型，等到Mapper.run调用上下文的nextKeyValue的时候，实际调用的是reader的nextKV。<br>那么reader是谁呢？回到<code>runNewMapper</code>方法，reader其实就是input，而input的类型就是解析输入文件的后缀名得到的；在wordcount示例里，输入是纯文本文件，实际就是TextInputFormat。<br><code>runNewMapper</code>的<code>NewTrackingRecordReader</code>调用了<code>TextInputFormat</code>的<code>createRecordReader</code>，最终创建了<code>LineRecordReader</code>对象。</p>
<p>答案就在LineRecordReader里。来看代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineRecordReader</span> <span class="keyword">implements</span> <span class="title">RecordReader</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">LineRecordReader</span><span class="params">(Configuration job, FileSplit split,	//record初始化方法</span><br><span class="line">      <span class="keyword">byte</span>[] recordDelimiter)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit genericSplit,</span><br><span class="line">                         TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">...</span><br><span class="line">    <span class="comment">// If this is not the first split, we always throw away first record</span></span><br><span class="line">    <span class="comment">// because we always (except the last split) read one extra line in</span></span><br><span class="line">    <span class="comment">// next() method.</span></span><br><span class="line">    <span class="keyword">if</span> (start != <span class="number">0</span>) &#123;		<span class="comment">//只要不是第一个分片，总是跳过第一行，因为前面的block处理的时候，已经越过分区读取完毕了</span></span><br><span class="line">      start += in.readLine(<span class="keyword">new</span> Text(), <span class="number">0</span>, maxBytesToConsume(start));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.pos = start;		<span class="comment">//记录本分split的起始位置</span></span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>) &#123;</span><br><span class="line">      key = <span class="keyword">new</span> LongWritable();</span><br><span class="line">    &#125;</span><br><span class="line">    key.set(pos);</span><br><span class="line">    <span class="keyword">if</span> (value == <span class="keyword">null</span>) &#123;</span><br><span class="line">      value = <span class="keyword">new</span> Text();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> newSize = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// We always read one extra line, which lies outside the upper</span></span><br><span class="line">    <span class="comment">// split limit i.e. (end - 1)</span></span><br><span class="line">    <span class="keyword">while</span> (getFilePosition() &lt;= end || in.needAdditionalRecordAfterSplit()) &#123;	<span class="comment">//保证到了split末尾时只会一次“超读”</span></span><br><span class="line">      <span class="keyword">if</span> (pos == <span class="number">0</span>) &#123;</span><br><span class="line">        newSize = skipUtfByteOrderMark();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));	<span class="comment">//答案：超读</span></span><br><span class="line">        pos += newSize;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>还记得刚开始的<code>runNewMapper</code>里是怎么处理的吗？对，先initialize，再run；run里不停的nextKeyValue。<br>具体到LineRecordReader，initialize的处理是，如果当前块不是首块，那么就会跳过第一行（Split的划分其实是逻辑上的，只是指定了该文件的start和end位置，而不是真实的划分成小文件），因为第一行已经在前面的块里处理了；<br>相应的，在NextKeyValue里，由于使用的是readLine，故而总是会读完该文件的一整行（而不是该split），如果是该行跨HDFS分区，那么就读取下一个分区。</p>
<p>答案其实很简单，不过借着这个问题，梳理了下MapTask的流程，虽然有点琐碎，但还是有所收获。<br>若分析谬误，还请指出:)</p>
<p>参考：<br><a href="http://my.oschina.net/xiangchen/blog/99653" target="_blank" rel="external">Hadoop MapReduce中如何处理跨行Block和InputSplit</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>前一篇文章解答了Map任务数、启动人的细节，下面我们解答第二个问题：<br>HDFS的block是否粗暴但忠实的将文件按照64MB分片呢？如果是的话，怎么保证Map获取到的Splits是正确的？具体到wordcount，MR是怎么处理一个单词跨block的情况呢？</p>]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[python学习作业（二）]]></title>
    <link href="http://yoursite.com/2015/05/28/python-homework-2/"/>
    <id>http://yoursite.com/2015/05/28/python-homework-2/</id>
    <published>2015-05-28T15:11:15.000Z</published>
    <updated>2015-05-28T15:51:30.000Z</updated>
    <content type="html"><![CDATA[<p>需求：生成纯文本格式的表数据，导入ODPS/ADS等。用户可以定义表的格式。<br>最早是想找一个类似DataFactory的工具来做，但今天问了下史进是自己用Java写的，走读了一遍逻辑不太复杂，于是花了些时间写了一个python版本的。</p>
<p>实现思路非常简单：配置文件使用json格式，python读取配置文件，按每一行的格式随机生成内容，每行生成后写入文件，最后关闭文件，结束。<br><a id="more"></a></p>
<p>几个有意思的点：</p>
<ul>
<li>json配置文件怎么写注释？json本身并没有定义注释，但是可以像本例一样加一个注释字段，占坑但不拉翔。</li>
<li>随机字符串的生成。<code>GenStr</code>的实现是从网上找到一个方法稍作修改：先随机生成若干字符，然后join。不知道是否有更高效的实现方法？</li>
<li>随机时间的生成，我这里的做法是取当前时间的浮点表示值，然后取比这个小的一个随机值，最后将其转为时间格式。</li>
<li>类似<code>switch case</code>的编码风格。python不支持switch，原因<a href="https://docs.python.org/2/faq/design.html#why-isn-t-there-a-switch-or-case-statement-in-python" target="_blank" rel="external">见这里</a>。我这里也是参考网上的一个做法，定义函数数组，效率比<code>if...elif...elif</code>应该高一点。</li>
</ul>
<p>配置文件cfg.json：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">filename</span>" : <span class="value"><span class="string">"test.txt"</span></span>,</span><br><span class="line">  "<span class="attribute">table</span>":</span><br><span class="line">  <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">lines</span>" : <span class="value"><span class="string">"30"</span></span>,</span><br><span class="line">    "<span class="attribute">_commet_for_columes</span>" : <span class="value"><span class="string">"colume type support BOOLEAN, BIGINT, DOUBLE, STRING, DATATIME only."</span></span>,</span><br><span class="line">    "<span class="attribute">columes</span>":<span class="value"><span class="string">"BIGINT, STRING, DATETIME, BOOLEAN, DOUBLE"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>生成数据gendata.py：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">staticChars = string.ascii_letters+string.digits</span><br><span class="line">staticCharLen = len(staticChars)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenBoolean</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> str(random.randint(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenBigInt</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> str(random.randint(<span class="number">0</span>, sys.maxint))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenDouble</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> str(random.uniform(<span class="number">0</span>, sys.maxint))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenStr</span><span class="params">()</span>:</span></span><br><span class="line">    length = random.randint(<span class="number">1</span>, staticCharLen)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([random.choice(staticChars) <span class="keyword">for</span> i <span class="keyword">in</span> range(length)])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenDate</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> time.strftime(<span class="string">"%Y-%M-%d"</span>, time.localtime(random.uniform(<span class="number">0</span>,time.time())))</span><br><span class="line"></span><br><span class="line">funcs = &#123;</span><br><span class="line">    <span class="string">"BOOLEAN"</span>: GenBoolean,</span><br><span class="line">    <span class="string">"BIGINT"</span> : GenBigInt,</span><br><span class="line">    <span class="string">"DOUBLE"</span> : GenDouble,</span><br><span class="line">    <span class="string">"STRING"</span> : GenStr,</span><br><span class="line">    <span class="string">"DATETIME"</span> : GenDate</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GenData</span><span class="params">(ctype)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> funcs[ctype.upper().strip()]()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    f = file(<span class="string">"cfg.json"</span>)</span><br><span class="line">    jscfg = json.load(f)</span><br><span class="line"></span><br><span class="line">    lines = int(jscfg[<span class="string">u'table'</span>][<span class="string">u'lines'</span>])</span><br><span class="line">    columes = str(jscfg[<span class="string">u'table'</span>][<span class="string">u'columes'</span>]).split(<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">    filep = file(jscfg[<span class="string">u'filename'</span>], <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">while</span> lines &gt; <span class="number">0</span>:</span><br><span class="line">        lines -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> type <span class="keyword">in</span> columes:</span><br><span class="line">            filep.write(GenData(type) + <span class="string">','</span>)</span><br><span class="line">        filep.write(<span class="string">'\n'</span>)</span><br><span class="line">    filep.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>需求：生成纯文本格式的表数据，导入ODPS/ADS等。用户可以定义表的格式。<br>最早是想找一个类似DataFactory的工具来做，但今天问了下史进是自己用Java写的，走读了一遍逻辑不太复杂，于是花了些时间写了一个python版本的。</p>
<p>实现思路非常简单：配置文件使用json格式，python读取配置文件，按每一行的格式随机生成内容，每行生成后写入文件，最后关闭文件，结束。<br>]]>
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[MapReduce具体问题（一）]]></title>
    <link href="http://yoursite.com/2015/05/25/mr-detail-1/"/>
    <id>http://yoursite.com/2015/05/25/mr-detail-1/</id>
    <published>2015-05-25T00:44:33.000Z</published>
    <updated>2015-05-28T15:53:52.000Z</updated>
    <content type="html"><![CDATA[<p>MapReduce比较基础，但是经常会有一些问题不是很清楚，这一系列文章会解答几个经常问的问题。<br>本文解答第一个问题：是谁决定要起几个Map任务？在什么阶段呢？<br><a id="more"></a></p>
<p>还是以wordcount为例。<br>wordcount客户端在初始化job后调用Job.waitForCompletion方法就结束了，真正提交Job给Yarn的在MapReduce客户端代码的submit里做的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span><br><span class="line">                                 )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span><br><span class="line">                                          ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span></span><br><span class="line">      return submitter.<span class="title">submitJobInternal</span><span class="params">(Job.<span class="keyword">this</span>, cluster)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span></span><br><span class="line">    <span class="comment">// Create the splits for the job</span></span><br><span class="line">    LOG.<span class="title">debug</span><span class="params">(<span class="string">"Creating splits at "</span> + jtFs.makeQualified(submitJobDir)</span>)</span>;</span><br><span class="line">    <span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br><span class="line">    conf.setInt(MRJobConfig.NUM_MAPS, maps);		<span class="comment">//确定MR任务数</span></span><br><span class="line">    LOG.info(<span class="string">"number of splits:"</span> + maps);</span><br></pre></td></tr></table></figure>
<p>从上面的代码不难看出，Map任务的个数是客户端提交任务到YARN之前就决定了的，其个数由writeSplits结果决定。<br>那么Splits是什么？跟HDFS有什么关系呢？</p>
<p>我们知道，HDFS存储大文件的时候，将文件按照每64MB（或者128MB，可以配置）分为若干个块（Block），每个Block又分了若干个副本，分别存储到不同的datanode上去。而在做MapReduce的时候，我们希望能够获得比较好的数据本地性，也就是说计算任务跟它需要的数据，最好在一个节点上，从而可以显著的减少数据在网络上的传输。<br>结合上面两点，不难得出一个结论，Map任务的个数（计算任务），实际应该是由MR处理的文件在HDFS上存储的块的个数决定的。</p>
<p>回过头来分析writeSplits，它调用了writeNewSplits。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobSubmitter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> &lt;T extends InputSplit&gt;</span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">writeNewSplits</span><span class="params">(JobContext job, Path jobSubmitDir)</span> <span class="keyword">throws</span> IOException,</span><br><span class="line">      InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    Configuration conf = job.getConfiguration();</span><br><span class="line">    InputFormat&lt;?, ?&gt; input =		<span class="comment">//先获取input的格式</span></span><br><span class="line">      ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line"></span><br><span class="line">    List&lt;InputSplit&gt; splits = input.getSplits(job);		<span class="comment">//根据格式获取分片数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">InputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span></span><br><span class="line">    List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext context</span><br><span class="line">                               )</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br></pre></td></tr></table></figure></p>
<p>writeNewSplits先获取input的格式（其实就是根据文件后缀名来判断的，例如gz, bzip2等等），然后再根据对应的格式对象里的getSplits来获取分片数。本例里wordcount要处理的就是一个“纯文本文件”，对应的FileInputFormat是InputFormat的一种。<br>下面我们以FileInputFormat来看getSplits是怎么获取分片数的。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FileInputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">InputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));	<span class="comment">//最小值，1</span></span><br><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);	<span class="comment">//最大值，Long.MAX_VALUE</span></span><br><span class="line">..</span><br><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;		<span class="comment">//如果输入可分片</span></span><br><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();		<span class="comment">//HDFS的block大小</span></span><br><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line"></span><br><span class="line">          <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">          <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                        blkLocations[blkIndex].getHosts(),		<span class="comment">//注意getHosts</span></span><br><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;	<span class="comment">//最后一个小分片</span></span><br><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class="line">                       blkLocations[blkIndex].getHosts(),</span><br><span class="line">                       blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable		//不可分片的输入</span></span><br><span class="line">          splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                      blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p>
<p>注意getSplits时有个“可分片”的概念，也就是isSplitable。分片的意思是，对于单独一个分片，可以独立的拿给对应的Map任务处理，分片之间无依赖。<br>是否所有的Input都是可以分片的呢？对于纯文本来说，从任何一个位置都是可以直接读取的，显然是可分片的；但是对于gz压缩的文件来说，如果我们只拿到了一个分片，是没办法只解压这一个分片的（因为gz压缩是将压缩信息存储到所有分片的，只有一个分片不能得到所有的压缩信息，无法解压）。hadoop的做法其实就是根据文件后缀名来查找codec，然后检查是否<code>codec instanceof SplittableCompressionCodec;</code>。isSplitable是可以@override的。<br><strong>可分片的Input处理方法：</strong></p>
<ul>
<li>确定分片大小。通过<code>file.getBlockSize()</code>获取HDFS的block大小，然后跟MR自己定义的最大最小值比较来选择一个中间值。MR的最大最小值默认是Long.MAX_VALUE/1，所在wordcount里确定的split大小实际就是HDFS的blck大小。</li>
<li>根据文件的总大小，先分整个split大小的，剩余的单独一个分片。每个分片都add到splits数组，add时会获取该block所在的节点地址（有缓存的时候优先使用缓存节点），提交Job后在分配Map任务时会参考这个节点地址，这样就可以将Map任务分配到数据所在的节点上（数据本地）。</li>
</ul>
<p><strong>对于不可分片的Input处理方法：</strong><br>只add一次，节点就是第一个分片的节点地址。所以对于不可分片的文件，其实是损失了MR的并行运行，多个分片也需要跨节点拷贝，效率比较低。</p>
<p>具体到wordcount，由于我们输入的文件是纯文本的，所以可以分片；而客户端会根据HDFS各个分片所在的节点地址分别添加到待启动的各个MR任务上，任务总数会写到<code>MRJobConfig.NUM_MAPS</code>，提交了以后使用。</p>
<p>至此，我们明确了Map任务个数的确定者以及其确定阶段。<br>下期预告：第二个问题也是大家经常问的一个，HDFS的block是否粗暴但忠实的将文件按照64MB分片呢？如果是的话，怎么保证Map获取到的Splits是正确的？具体到wordcount，MR是怎么处理一个单词跨block的情况呢？</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>MapReduce比较基础，但是经常会有一些问题不是很清楚，这一系列文章会解答几个经常问的问题。<br>本文解答第一个问题：是谁决定要起几个Map任务？在什么阶段呢？<br>]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[读书笔记：hive简介（一）]]></title>
    <link href="http://yoursite.com/2015/05/20/hive-intro-1/"/>
    <id>http://yoursite.com/2015/05/20/hive-intro-1/</id>
    <published>2015-05-20T01:16:04.000Z</published>
    <updated>2015-05-21T11:01:27.000Z</updated>
    <content type="html"><![CDATA[<p>Hive是一个运行在hadoop之上的数据仓库(dataware)，目的是为精通SQL但不熟悉java的分析师提供一个在hadoop平台上数据分析的工具。<br>Hive提供类似SQL的接口，但并不完全相同：Hive没有完全实现SQL的所有语法，但又结合MR任务提供了一些新的方法。分析师按照HQL的语法提交任务，Hive将HQL语句翻译为MR作业。</p>
<a id="more"></a>
<h3 id="1_安装">1 安装</h3><p>跟以前一样，我是用Cloudera Manager工具安装的hive。CM可以很方便删除、添加某一服务，安装时可以选择服务运行在哪一台具体设备上。CM界面虽然比ambari丑了一点，但是功能强大很多。需要注意impala/hue/oozie都依赖hive。<br>我安装了这几个服务：</p>
<ul>
<li>Hive Metastore Server，元数据服务器。配置时需要指定元数据使用的数据库是什么，我这里指定了psql。</li>
<li>HiveServer2，thrift服务器，通过JDBC/ODBC可以提供命令行之外的编程接口，如使用java、python等。</li>
<li>WebHCat Server，Hcatalog的REST API服务器。<br><img src="http://7xir15.com1.z0.glb.clouddn.com/hive架构.jpg" alt="hive架构"></li>
</ul>
<h4 id="1-1_hive外壳环境">1.1 hive外壳环境</h4><p>外壳环境是一个交互式的、通过HiveQL与hive交互的工具。hive官方推荐使用beeline（参加上面图，beeline是JDBC上的工具）作为交互式工具，由于还不太熟悉，下面我们还是以CLI(<code>hive</code>命令)为shell。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br><span class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: <span class="number">1.512</span> seconds, Fetched: <span class="number">1</span> row(s)</span><br></pre></td></tr></table></figure></p>
<p>hive外壳环境有如下三种使用方法：</p>
<ul>
<li>可以直接在shell中使用hiveQL进行交互式的数据库操作。</li>
<li>可以执行脚本：创建一个文件hive.q，将HQL语句写到hive.q中，然后通过<code>hive -f hive.q</code>执行该文件。</li>
<li>可以执行内嵌语句：<code>hive -e &#39;show tables;&#39;</code></li>
</ul>
<h4 id="1-2_示例">1.2 示例</h4><p>下面我们用网上开房数据作为示例，演示下hive是怎么操作的(此处较水)。<br><strong>a 创建一个表record2</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table records2(name STRING, cardno INT, descri STRING, ctftp STRING, ctfid STRING, gender STRING, birth INT)</span><br><span class="line">    &gt; row format delimited</span><br><span class="line">    &gt; fields terminated by <span class="string">','</span>;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p>
<p>解释两点：<br>第一点，该数据一共有30+的column，但实际我们可能并不想分析这么多，hive支持创建一个只有前面几列的表。<br>第二点是后面两行。这个数据是csv格式，实际就是文本格式（如下），创建表的时候我们需要指定数据格式：行记录之间用换行区分（即<code>row format delimited</code>），每一行内各段用逗号区分（即<code>fields terminated by &#39;,&#39;</code>）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ head <span class="number">5000</span>.csv</span><br><span class="line">﻿Name,CardNo,Descriot,CtfTp,CtfId,Gender,Birthday,Address,Zip,Dirty,District1,District2,District3,District4,District5,District6,FirstNm,LastNm,Duty,Mobile,Tel,Fax,EMail,Nation,Taste,Education,Company,CTel,CAddress,CZip,Family,Version,id</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p><strong>b 将本地文件上传加载到hive</strong><br>实际上这里只是文件的复制，将5000.csv复制到了hdfs的<code>/user/hive/warehouse/records2/</code>目录中去（所以hive是个数据仓库）。<br>指定overwrite可以覆盖原来的table。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data <span class="built_in">local</span> inpath <span class="string">'5000.csv'</span></span><br><span class="line">    &gt; overwrite into table records2;</span><br><span class="line">Loading data to table default.records2</span><br></pre></td></tr></table></figure></p>
<p><strong>c 查询</strong><br>我们查一下去开房的年龄最小的是多大。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select MAX(birth) from records2 <span class="built_in">where</span> (birth &gt;<span class="number">10000000</span> AND birth &lt; <span class="number">20000000</span>);</span><br><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">18</span> <span class="number">05</span>:<span class="number">24</span>:<span class="number">16</span>,<span class="number">328</span> Stage-<span class="number">1</span> map = <span class="number">0</span>%,  reduce = <span class="number">0</span>%</span><br><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">18</span> <span class="number">05</span>:<span class="number">24</span>:<span class="number">23</span>,<span class="number">631</span> Stage-<span class="number">1</span> map = <span class="number">100</span>%,  reduce = <span class="number">0</span>%, Cumulative CPU <span class="number">1.84</span> sec</span><br><span class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">18</span> <span class="number">05</span>:<span class="number">24</span>:<span class="number">28</span>,<span class="number">765</span> Stage-<span class="number">1</span> map = <span class="number">100</span>%,  reduce = <span class="number">100</span>%, Cumulative CPU <span class="number">3.03</span> sec</span><br><span class="line">MapReduce Total cumulative CPU time: <span class="number">3</span> seconds <span class="number">30</span> msec</span><br><span class="line">Ended Job = job_1431517711165_0014</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage-Stage-<span class="number">1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">3.03</span> sec   HDFS Read: <span class="number">7495191</span> HDFS Write: <span class="number">9</span> SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: <span class="number">3</span> seconds <span class="number">30</span> msec</span><br><span class="line">OK</span><br><span class="line"><span class="number">20130101</span></span><br></pre></td></tr></table></figure></p>
<p>查到年龄最小的是2013年出生的小baby，名字叫做123。虽然我在查找的时候已经考虑到了有些数据统计不准确，把出生日期做了限制，但还是查出来一条不太有用的数据。不知道这是不是就是需要“数据清洗”了？</p>
<h3 id="2_hive运行解析">2 hive运行解析</h3><h4 id="2-1_配置">2.1 配置</h4><p>配置属性按照下面的的顺序优先级依次递减：</p>
<ul>
<li><p>hive shell中SET命令。配置项的值查看也是用SET，只是不要带参数。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="built_in">set</span> hive.metastore.warehouse.dir;</span><br><span class="line">hive.metastore.warehouse.dir=/user/hive/warehouse</span><br></pre></td></tr></table></figure>
</li>
<li><p>hive命令行中<code>-hiveconf  configfile</code></p>
</li>
<li>hive-site.xml</li>
</ul>
<h4 id="2-2_日志">2.2 日志</h4><p>hive使用log4j。</p>
<h4 id="2-3_hive服务端">2.3 hive服务端</h4><ul>
<li>cli：也就是我们hive进去的交互式命令行接口。</li>
<li>hive server 2：thrift服务器。</li>
<li>metastore server：元数据服务器。</li>
<li>hive web interface:使用web接口代替命令行。我这里没有安装。</li>
</ul>
<h4 id="2-4_客户端">2.4 客户端</h4><p>thrift客户端：支持在C++/python/ruby/PHP/java等语言中嵌入hive命令。<br>JDBC驱动：提供给java应用程序访问hive的方法。<br>ODBC驱动：ODBC程序通过ODBC驱动访问hive。</p>
<h4 id="2-5_metastore">2.5 metastore</h4><p>metastore存放hive的元数据，主要做两件事：服务，数据存储。metastore有如下几种方式：</p>
<ul>
<li>与hive服务在同一进程，使用内嵌数据库（如derby）。问题是对于一个metastore来说只能有一个hive会话，只能单用户。</li>
<li>与hive服务在同一进程，但使用独立数据库（如mysql、psql、oracle）。可以支持多用户（多会话），但有耦合。</li>
<li>metastore作为一个独立的进程，可以部署在其他机器上；使用独立数据库。CM部署就是采用了这种模式。另外如果要用impala，不能使用derby。</li>
</ul>
<h3 id="3_与传统数据库比较">3 与传统数据库比较</h3><p>hive底层依赖HDFS，导致与传统数据库有不同。</p>
<h4 id="3-1_读时模式_vs_写时模式">3.1 读时模式 vs 写时模式</h4><p>传统数据库采用的是写时模式，即数据写入数据库的时候，就会检查其是否符合数据库的模式（例如数据库定义本列为INT，传入STRING会报错）。<br>hive采用的是读时模式。数据写入数据库的时候并不检查模式是否合法，直到查询的时候才会检查。</p>
<p>各有各的优势与应用场景。<br>传统数据库的写时模式可以提高查询性能，写入时也可以做压缩；但欠缺灵活，有时数据库写入时不能确定使用什么模式。<br>hive的读时模式有两个优势：一是在写入数据库的时候速度很快，因为只是文件复制，不需要解析数据；二是更灵活，对于同一份数据可以支持多个模式。hive支持外部表，即数据与表分离，一份数据可以创建多个外部表，表删除时<code>drop table xxx</code>数据不会删除，删除的只是元数据。</p>
<h4 id="3-2_更新、事务和索引">3.2 更新、事务和索引</h4><p>《权威指南》里指出hive因为强调为整个数据的处理，所以并没有更新这种需求。但实际还是需要追加、索引这些功能的，应该还是会逐渐补充进来的。</p>
<blockquote>
<p>update:<br>@史进: hive 0.7之后有了索引功能，0.14之后有了插入功能。</p>
</blockquote>
<p>另外，通过将hive和hbase集成后，可以获得传统数据库的这些特性，详细可以查看<a href="https://cwiki.apache.org/confluence/display/Hive/HBaseintegration" target="_blank" rel="external">这里</a>。CSDN上有一篇<a href="http://blog.csdn.net/aaronhadoop/article/details/28398157" target="_blank" rel="external">文章</a>，详细写了整合以后的效果，可以参考。</p>
<h3 id="4_HiveQL">4 HiveQL</h3><p>hiveQL是SQL的一种方言，并没有完整的支持SQL-92的所有特性，其目标更多的还是为了实际使用。但由于基于MR，hive还提供了SQL-92之外的特性，例如多表插入和transform、map、reduce等子句。</p>
<p>hiveQL支持两类数据类型：</p>
<ul>
<li>原子数据类型：<br>TINYINT,SMALLINT,INT,BIGINT,BOOLEAN,FLOAT,DOUBLE,STRING,BINARY,TIMESTAMP,DECIMAL,CHAR,VARCHAR,DATE。原子数据类型支持隐性类型转换，其原则为往表达范围更大的方向转换；也可以使用CAST子句显式转换，如<code>CAST &#39;1&#39; AS INT</code>将字符1转换为整型的1。</li>
<li>复杂类型：ARRAY,MAP,STRUCT,UNION</li>
</ul>
<p>有关各类型的详细信息请参考<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types" target="_blank" rel="external">hive wiki</a></p>
<blockquote>
<p>挖坑不止， 未完待续</p>
</blockquote>
<p>参考：<br><a href="http://database.51cto.com/art/201407/446692.htm" target="_blank" rel="external">51cto：Hive已为Hadoop带来实时查询机制</a><br>《hadoop权威指南》</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Hive是一个运行在hadoop之上的数据仓库(dataware)，目的是为精通SQL但不熟悉java的分析师提供一个在hadoop平台上数据分析的工具。<br>Hive提供类似SQL的接口，但并不完全相同：Hive没有完全实现SQL的所有语法，但又结合MR任务提供了一些新的方法。分析师按照HQL的语法提交任务，Hive将HQL语句翻译为MR作业。</p>]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop文件系统：HDFS]]></title>
    <link href="http://yoursite.com/2015/05/16/hdfs-intro/"/>
    <id>http://yoursite.com/2015/05/16/hdfs-intro/</id>
    <published>2015-05-15T16:56:26.000Z</published>
    <updated>2015-05-20T01:54:04.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1_概述">1 概述</h3><p>HDFS应用场景：存储超大型流式处理数据（Terabytes和Petabytes级别）。<br>总的来说，HDFS的特点有这么几个：</p>
<ul>
<li>“write once, read many”，只支持一个writer，但对并发的reader支持很高的吞吐。</li>
<li>将数据处理逻辑放置到数据附近，可以减少数据拷贝的成本，提高并发效率。下面的读取/写入，都体现了这个特点。</li>
<li>可靠：维护同一文件的的多个副本+故障发生时自动重新部署问题节点。</li>
</ul>
<a id="more"></a>
<p>从这几个不能满足的要求，可以反过来看HDFS的特点：</p>
<ul>
<li>低延迟的数据访问。HDFS关注的是数据吞吐量，强调整个文件。</li>
<li>大量的小文件。HDFS设计为支持大文件，大量的小文件会造成namenode负载沉重。</li>
<li>多用户写入，任意修改文件。只支持一个writer，且只能写到文件的最后（流式处理）。</li>
<li>随机数据访问。</li>
</ul>
<h3 id="2_架构">2 架构</h3><ul>
<li>name node：管理文件系统命名空间和访问权限，记录了data node的数据块的位置（注意是在内存里，不保存，每次data node加入时重建）。</li>
<li>data node：将数据作为块存储在文件里。</li>
</ul>
<p><img src="http://www.ibm.com/developerworks/cn/web/wa-introhdfs/fig1.gif" alt=""><br>HDFS架构如上图。name node可以识别data node的机架ID，从而优化data node之间的通信，减少跨机架的通信，因为通常这比同一机架的通信要慢。</p>
<h4 id="块">块</h4><p>类似传统文件系统，HDFS也有<strong>块</strong>的概念，默认为64MB，但通常会被设置为128MB（跟硬盘读写速度相关）。数据存储到块文件里去。<br>块如此之大的原因：HDFS读取的吞吐率取决于2个方面：寻址的速度、块读取的速度。块设计的大一些，可以最小化寻址开销，尽可能接近硬盘的读取速度（极限情况：设计为只有一块，完全是硬盘的读取速度）。但不能将块设置太大，否则数据块的个数变少，会造成Map任务并发不够。<br>att：如果块存放的数据比块要小，实际并没有完全占用块。<br>使用块的优点：可以支持超大文件（经历过1080P拷贝到FAT32移动硬盘的同学会懂其中的痛）；相对管理文件来说，管理块更简单；可以做容错、负载分担。</p>
<h4 id="文件访问权限">文件访问权限</h4><p>如下例，hdfs跟POSIX非常像。hdfs的用户，实际就是远程客户端操作时候的用户，比如下面我用test上传了一个文件上去，其用户就是test，不管其他节点上是否也有test用户。hdfs的’x’权限，只是表示”该目录可进入“，没有”执行“这个概念。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="built_in">test</span>@master conf]$ hadoop fs -put core-site.xml  /tmp/</span><br><span class="line">[<span class="built_in">test</span>@master conf]$ hadoop fs -ls /tmp/</span><br><span class="line">Found <span class="number">5</span> items</span><br><span class="line">drwxrwxrwx   - hdfs   supergroup          <span class="number">0</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">11</span> <span class="number">22</span>:<span class="number">28</span> /tmp/.cloudera_health_monitoring_canary_files</span><br><span class="line">-rw-r--r--   <span class="number">3</span> <span class="built_in">test</span>   supergroup       <span class="number">3849</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">14</span> <span class="number">19</span>:<span class="number">10</span> /tmp/core-site.xml</span><br><span class="line">..</span><br></pre></td></tr></table></figure></p>
<p>新版本的hadoop使用了kerberos来做认证。</p>
<h3 id="3_文件读取">3 文件读取</h3><h4 id="3-1_访问接口">3.1 访问接口</h4><p>hdfs提供了多种访问方式。<br>方式一：我们在客户端使用<code>hadoop fs -ls /</code>的操作，实际就是一个应用程序调用hdfs的java api接口实现的。<br>方式二：hdfs还提供了一个libhdfs的C语言库，可以通过JNI来访问；其他一些语言，例如c++, ruby, python等等可以通过<a href="http://dongxicheng.org/search-engine/thrift-framework-intro/" target="_blank" rel="external">thrift</a>代理来访问。thrift是个比较神奇的东西，后面可以好好玩一下。<br>方式三：hdfs提供了web浏览服务，下文提到的distcp就是利用了这一方式，其优势在于没有版本兼容的顾虑。</p>
<h4 id="3-2_客户端读取HDFS过程">3.2 客户端读取HDFS过程</h4><p><img src="http://upload.news.cecb2b.com/2014/1108/1415426527715.jpg?_=42872" alt=""><br>这个过程中，name node负责处理block位置的请求，客户端获得位置信息后，直接与data node联系，避免name node称为瓶颈。读取有下面两个特点</p>
<ul>
<li>距离最近。读取datanode时，有一个“距离最近”的概念。name node返回存储该块的data node列表，dfs的datanode管理对象会从距离最近的datanode读取该数据块。该块read完毕后，继续请求下一块数据，直到读取完毕。</li>
<li>容错。datanode管理对象会校验读取数据，如果出错，会从其他最近的节点重新读取块，并且在读取下一block时通知name node。</li>
</ul>
<p>那么距离是怎么定义的呢？可以将网络看成一棵树，两个节点的距离就是他们到最近的共同祖先的距离之和。根据数据中心、机架、节点，可以定义不同层级；对于复杂的网络，需要用户帮助hadoop来定义其拓扑。</p>
<h3 id="4_文件写入">4 文件写入</h3><p>参考下面的代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.fs.FileSystem hdfs = org.apache.hadoop.fs.FileSystem.get(config);</span><br><span class="line">org.apache.hadoop.fs.Path path = <span class="keyword">new</span> org.apache.hadoop.fs.Path(filePath);</span><br><span class="line">org.apache.hadoop.fs.FSDataOutputStream outputStream = hdfs.create(path);</span><br><span class="line">outputStream.write(fileData, <span class="number">0</span>, fileData.length);</span><br></pre></td></tr></table></figure></p>
<p>首先在name node文件系统的命名空间里创建一个新文件，注意这时还没有真正的数据块（实际就是一个记录）。<br>然后，数据被分为一个个数据包，一次性写入“数据队列”；数据队列的处理器会向name node请求得到一组data node作为一个“管道”；数据流式写入管道的第一个data node，然后再由这个节点同步给其他data node。<br><img src="http://upload.news.cecb2b.com/2014/1108/1415426527902.jpg?_=6898" alt=""><br>只要有一个datanode写入成功就可以，集群会自行异步复制到其他节点。</p>
<blockquote>
<p>问题：“管道”申请是每文件，还是每block？</p>
</blockquote>
<p>name node是怎么选择data node来做副本写入呢？布局策略有很多，默认策略是这样的：先在写入数据的客户端本地存储一份（如果客户端不在HDFS集群上，则随机选择一个集群节点），然后在其他另一机架上选择2个节点各存储一份，再有副本就随机选择了。<strong>只要写入一个data node成功，写操作就会成功，客户端调用只需要等待最小量的复制。</strong><br>该策略很好的平衡了可靠性（跨机架）、写入带宽（只需考虑本地交换机）、读取带宽（两个机架），数据均匀分布。</p>
<p><strong>数据一致性</strong><br>创建文件后，命名空间立即可见；但写入数据不能立即可见。应用程序需要自己权衡写入吞吐量和鲁棒，自行决定什么时候sync同步缓存。另外，文件关闭<code>close()</code>隐含了sync方法，需要所有数据写入datanode才能确认返回。</p>
<p>这里再谈一下客户端提交数据的过程。考虑这样一个问题：客户端读取小的日志并写入hdfs，每读取一条调用write，是否就引起写入hdfs一次？实际上客户端会建立一个临时本地文件，写入数据会被重定向到这个临时文件，直到满足一个数据块的大小；之后关闭临时文件，才会真正将此数据提交到hdfs上。</p>
<h3 id="5_可靠">5 可靠</h3><p><strong>data node心跳</strong><br>data node需要有间隔的向name node发送心跳，如果心跳丢失，name node将该data node设置为不可用，如果导致副本低于<strong>复制因子</strong>，还需要将该data node上的数据块复制到其他的data node上。</p>
<p><strong>数据块再平衡</strong><br>情景一：当某些data node上的空闲空间太小时，hdfs会自动移动。<br>情景二：新增data node时。可以考虑手动平衡<code>hadoop balance</code>。</p>
<p><strong>数据完整</strong><br>对hdfs上的文件会存储checksum到文件系统命名空间的隐藏文件中；客户端取到数据后会检验这个checksum是否正确。</p>
<p><strong>元数据同步</strong><br>我们需要先了解几个概念，否则下面要讲的就完全看不懂了。<br>EditLog：一个记录文件系统命名空间的变换，例如文件重命名，权限修改，文件创建，数据块定位等等的持久化的文件。修改玩EditLog后，客户端的调用才会返回。<br>fsImage：实际就是文件系统周期性的还原点，也是持久化的。在fsImage基础之上，回放EditLog，可以得到最新的文件系统。<br>数据块位置记录：跟上面两个不同，存放在内存。name node启动时根据data node的报告重建。</p>
<p>简单的来说，name node的文件系统命名空间，权限，文件与数据块的映射关系，存储在fsImage文件中；所有用户对hdfs的操作，存放在EditLog文件中。hdfs对这些元数据文件可以做多副本（如写到远端NFS）。</p>
<p><strong>secondary namenode</strong><br>与namenode不同，主要工作是将命名空间fsImage和EditLog融合，并记录融合后的image，防止EditLog过大。其EditLog的同步滞后于name node。<br>name node故障后的过程如下：</p>
<ul>
<li>将NFS上的image拷贝到secondary namenode上，加载到内存</li>
<li>重放EditLog记录的操作</li>
<li>收集足够的data node报告，重新建立数据块的映射关系</li>
<li>离开safe mode，成为正式的name node。</li>
</ul>
<p>这一过程在大集群中可能会到几十分钟。</p>
<p><strong>HA</strong><br>hadoop 2.x版本比较好的解决了name node的单点故障。<br>新版本引入了name node<strong>对</strong>，分别配置为master/standby。故障恢复需要的两个关键信息是这样处理的：<br><img src="http://pic002.cnblogs.com/images/2012/402771/2012120917500498.png" alt=""></p>
<ul>
<li>EditLog存放在NFS一类的共享存储中。master故障后，standby会立即重放EditLog，快速恢复状态。standby仍然运行着secondary namenode，用于融合image和EditLog。</li>
<li>datanode需要向master/standby同时报告，两个name node都有完整的数据块映射。</li>
</ul>
<p>据说只要几十秒就可以完成倒换。</p>
<h3 id="6_Fedoration">6 Fedoration</h3><p>引入了”域“的概念，允许集群不止有一个name node，但不同的name node共享同一个data node集群。<br>Fedoration可以解决集群变大时，name node成为瓶颈的问题，但是name node仍然存在单点故障，应属于一个过渡方案。不过Fedoration引入域以后，意外的获得了多租户的特性。</p>
<h3 id="7_小文件解决方案">7 小文件解决方案</h3><p><strong>归档工具</strong><br>archive可以克服大量小文件给namenode带来的内存压力，并且还能获得文件的透明访问。archive实际也是一个mr任务，需要hadoop集群。<br>[hdfs@master conf]$  hadoop archive -archiveName aaa.har -p /user/hdfs /user/hdfs<br>归档过程中是可以对文件进行压缩的，但是归档文件本身是不能压缩的。归档文件创建后不可更改，如果要删除或修改其中文件，只能重新归档。<br><strong>Sequence file</strong><br>由一系列KV组成，key为文件名，value为文件内容，将大批kv组成一个大文件。<br>这两种方案都需要用户应用程序干预，hdfs不会干预。</p>
<h3 id="8_distcp">8 distcp</h3><p>distcp是一个分布式并行的拷贝工具，实质是一个只有map的MR任务。通过制定map任务数(-m)，可以并行的将源文件拷贝到目标文件中。<br>相同版本的hdfs，distcp可以直接RPC拷贝；不同版本，则只能在目标hadoop上，通过http协议访问源dfs来并行的读取写入。<br><em>数据平衡性</em>：如果distcp -m 1，当文件很大的时候，由于只有一个map任务，那么写入的时候总是会写入该map任务所在的节点，这就带来不均衡，所以尽量选择更多的map任务。</p>
<p>最后回答前面的问题：是每block的，因为hdfs的目的就是为了存储大文件，如果是每文件，就只能固定存在3个节点上了。</p>
<p>参考：<br>Hadoop权威指南<br><a href="http://www.ibm.com/developerworks/cn/web/wa-introhdfs/" target="_blank" rel="external">IBM Developerworks</a><br><a href="http://www.cnblogs.com/beanmoon/archive/2012/12/11/2809315.html" target="_blank" rel="external">CSDN社区</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="1_概述">1 概述</h3><p>HDFS应用场景：存储超大型流式处理数据（Terabytes和Petabytes级别）。<br>总的来说，HDFS的特点有这么几个：</p>
<ul>
<li>“write once, read many”，只支持一个writer，但对并发的reader支持很高的吞吐。</li>
<li>将数据处理逻辑放置到数据附近，可以减少数据拷贝的成本，提高并发效率。下面的读取/写入，都体现了这个特点。</li>
<li>可靠：维护同一文件的的多个副本+故障发生时自动重新部署问题节点。</li>
</ul>]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://yoursite.com/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark（一）：介绍、初体验]]></title>
    <link href="http://yoursite.com/2015/05/13/spark-intro/"/>
    <id>http://yoursite.com/2015/05/13/spark-intro/</id>
    <published>2015-05-13T03:18:55.000Z</published>
    <updated>2015-05-13T06:09:40.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1_介绍">1 介绍</h3><p>Spark是一个快速、通用的集群计算系统，提供JAVA/Scala/Python API，以及一系列的高级工具：Spark SQL/MLib/GrapyX/Spark Streaming.<br>Spark的编程语言是scala，同样采用scala的还有kafka。<br><a id="more"></a></p>
<h3 id="2_安装">2 安装</h3><p>我的环境使用的是CDH版本，安装时选上了Spark，手动安装请参考官网。<br>如果将Spark安装到hadoop上，需要注意其版本依赖关系。如果你的Spark底层使用了HDFS做存储，而Spark的版本与默认的hadoop一定要不同的话，需要自行编译，改脚本没用。</p>
<h3 id="3_核心：RDD">3 核心：RDD</h3><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)模型的出现主要为了解决下面两个场景：</p>
<ul>
<li>迭代计算</li>
<li>交互式数据挖掘：同一数据子集进行Ad-hoc计算</li>
</ul>
<p>RDD解决了中间结果重用的问题，不再像MR模型必须写入HDFS。RDD具有下面两个特点：</p>
<ul>
<li>分布式：分布在多个节点上，可以被并行处理；存储在HDFS或者RDD数据集，也可以缓存在内存中，从而被多个并行任务重用。</li>
<li>容错：某个节点挂掉后，丢失的RDD可以重构。</li>
</ul>
<p>RDD支持两种操作：</p>
<ul>
<li>转换。从现有RDD生成新的RDD，例如<code>map(func)</code>、<code>filter(func)</code>、<code>join()</code>等。</li>
<li>动作。将操作结果返回驱动程序或者写入存储，例如<code>reduce(func)</code>、<code>count()</code>、<code>saveAsTextFile()</code></li>
</ul>
<p>RDD还支持缓存，主要用在迭代计算中，转换时不用<em>再次计算</em>，用户可以用persist/cache等方法使中间结果的RDD数据集缓存在内存或磁盘中。<br>有关RDD的详细研究，可以参考CSDN的<a href="http://blog.csdn.net/wwwxxdddx/article/details/45647761" target="_blank" rel="external">这篇文章</a>。</p>
<h3 id="4_基本架构">4 基本架构</h3><p>Spark会将一个应用程序转为一组任务，分布在多个节点并行计算，并提供一个共享变量在这些并行任务间、任务与驱动程序间共享。<br>每个应用程序一套运行时环境，其生命周期如下：</p>
<ul>
<li>准备运行时环境：资源管理。目前spark可以使用mesos（粗细两种粒度）/yarn（仅粗粒度）来作为其资源管理器。两种方式都需要BlockManager来管理RDD缓存。</li>
<li>将任务转换为DAG图。RDD父子数据集间存在宽依赖、窄依赖。连续多个窄依赖可以归并到一个阶段并行。</li>
<li>根据调度依赖关系执行DAG图。优化：数据本地性、推测执行。</li>
<li>销毁运行时环境</li>
</ul>
<h3 id="5_Quick_start">5 Quick start</h3><p>下面基本是参照<a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">spark官网示例</a>来的。</p>
<h4 id="5-1_spark_shell">5.1 spark shell</h4><p>spark shell可以交互式的运行spark程序，可以查看中间运行结果，是个学习框架的好方法。<br><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$ spark-shell</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _<span class="command">\ </span><span class="command">\/</span> _ <span class="command">\/</span> _ `/ __/  '_/</span><br><span class="line">   /___/ .__/<span class="command">\_</span>,_/_/ /_/<span class="command">\_</span><span class="command">\ </span>  version 1.3.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_15)</span><br><span class="line">...</span><br><span class="line">scala&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>我们可以在shell中进行一些scala的操作。shell启动时会自动提供一个SparkContext对象，我们可以直接用这个对象来加载文件为RDD。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(<span class="string">"pi.py"</span>)		<span class="comment">#textFile为一个RDD</span></span><br><span class="line">textFile: org.apache.spark.rdd.RDD[String] = pi.py MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">scala&gt; textFile.count()		<span class="comment">#RDD的action</span></span><br><span class="line">res4: Long = <span class="number">41</span></span><br><span class="line">scala&gt; textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>)).count()</span><br><span class="line">res7: Long = <span class="number">2</span>				<span class="comment">#RDD的transformation+action，先生成</span></span><br><span class="line"><span class="number">15</span>/<span class="number">05</span>/<span class="number">12</span> <span class="number">19</span>:<span class="number">00</span>:<span class="number">25</span> INFO DAGScheduler: Job <span class="number">2</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.184386</span> s</span><br></pre></td></tr></table></figure></p>
<p>spark也提供了一个python语言的shell，运行<code>pyspark</code>可以进去，使用起来跟scala类似，具体可以参见官网。</p>
<p>RDD提供了跨集群、内存级的<em>cache</em>功能，对于一些频繁访问的数据集生成缓存，可以提高效率。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>))</span><br><span class="line">scala&gt; linesWithSpark.collect()</span><br><span class="line">res12: Array[String] = Array(from pyspark import SparkContext, <span class="string">"    sc = SparkContext(appName="</span>PythonPi<span class="string">")"</span>)</span><br><span class="line">scala&gt; linesWithSpark.cache()</span><br><span class="line">res13: linesWithSpark.type = MapPartitionsRDD[<span class="number">5</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line">res16: Long = <span class="number">2</span></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line">res17: Long = <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h4 id="5-2_Self-Contained_Applications">5.2 Self-Contained Applications</h4><p>实际生产环境中不可能像上面这样交互式运行，还是要自包含的应用程序。下面我们举一个源代码里python的例子，scala和java需要配合sbt和maven，具体请参照官网。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="string">"""</span><br><span class="line">        Usage: pi [partitions]</span><br><span class="line">    """</span></span><br><span class="line">    sc = SparkContext(appName=<span class="string">"PythonPi"</span>)			<span class="comment">#sc要自己创建</span></span><br><span class="line">    partitions = int(sys.argv[<span class="number">1</span>]) <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">    n = <span class="number">100000</span> * partitions</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(_)</span>:</span></span><br><span class="line">        x = random() * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">        y = random() * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> x ** <span class="number">2</span> + y ** <span class="number">2</span> &lt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    count = sc.parallelize(xrange(<span class="number">1</span>, n + <span class="number">1</span>), partitions).map(f).reduce(add)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Pi is roughly %f"</span> % (<span class="number">4.0</span> * count / n)</span><br><span class="line"></span><br><span class="line">    sc.stop()	<span class="comment">#停掉sc</span></span><br></pre></td></tr></table></figure></p>
<p>简单来说，spark应用程序比shell方式多了SparkContext的创建、销毁，其他基本是一致的。<br>使用spark-submit将此python脚本提交到spark执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit pi.py <span class="number">10</span></span><br><span class="line"><span class="number">15</span>/<span class="number">05</span>/<span class="number">12</span> <span class="number">19</span>:<span class="number">42</span>:<span class="number">26</span> INFO DAGScheduler: Job <span class="number">0</span> finished: reduce at /opt/cloudera/parcels/CDH-<span class="number">5.4</span>.<span class="number">0</span>-<span class="number">1</span>.cdh5.<span class="number">4.0</span>.p0.<span class="number">27</span>/lib/spark/examples/lib/pi.py:<span class="number">38</span>, took <span class="number">1.867190</span> s</span><br><span class="line">Pi is roughly <span class="number">3.142040</span></span><br></pre></td></tr></table></figure></p>
<p>如果python程序依赖其他的文件或第三方的lib库，可以将其打包为zip文件，用—py-files指定就可以了。python系统库不需要。</p>
<p>Spark提供了一个history web server，可以看到我们运行了的那些程序以及他们的详细信息。<br><img src="http://7xir15.com1.z0.glb.clouddn.com/spark_jobs.PNG" alt="spark jobs"></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="1_介绍">1 介绍</h3><p>Spark是一个快速、通用的集群计算系统，提供JAVA/Scala/Python API，以及一系列的高级工具：Spark SQL/MLib/GrapyX/Spark Streaming.<br>Spark的编程语言是scala，同样采用scala的还有kafka。<br>]]>
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[流式计算框架Storm介绍]]></title>
    <link href="http://yoursite.com/2015/05/11/storm-intro/"/>
    <id>http://yoursite.com/2015/05/11/storm-intro/</id>
    <published>2015-05-11T02:56:25.000Z</published>
    <updated>2015-05-13T06:20:14.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1_背景：MR的问题">1 背景：MR的问题</h3><ul>
<li>启动时间长。多采用pull模型，没有JVM缓存池</li>
<li>调度开销大</li>
<li>中间数据写磁盘</li>
</ul>
<p>storm的出现，可以比较好的解决上面的问题。<br><a id="more"></a></p>
<h3 id="2_Storm的优势">2 Storm的优势</h3><p>实时计算、流式计算。水管不停的产生数据，流向中间的螺栓(处理逻辑)。<br><img src="http://tech.uc.cn/wp-content/uploads/2013/09/topology%E4%BE%8B%E5%AD%902.jpg" alt="stom模型"><br>Storm出现之前的解决方法：消息队列，读取消息队列，更新数据库，通知其他消息队列，存在缺点：自动化、健壮性、伸缩性。可以参考知乎的一个<a href="http://www.zhihu.com/question/20028515" target="_blank" rel="external">问答</a>：</p>
<blockquote>
<p>问：实时处理系统（类似s4, storm）对比直接用MQ来做好处在哪里？<br>答：好处是它帮你做了： 1) 集群控制。2) 任务分配。3) 任务分发 4) 监控 等等。</p>
</blockquote>
<p>总结Storm的优势：</p>
<ul>
<li>分布式：只要修改并发任务数，就可以获得更好的分布式性能</li>
<li>运维简单</li>
<li>高度容错：模块无状态，随时可重启</li>
<li>无数据丢失：ack消息追踪记录</li>
<li>多语言编程接口：貌似还是以java为主</li>
</ul>
<h3 id="3_编程模型">3 编程模型</h3><p>Tuple：数据表示模型，数据库中的一行记录，可以为integer、long，也可以自定义的序列化。<br>Stream：消息流。每个Tuple认为是一个消息，消息流就是Tuple队列。<br>Topology：应用程序处理逻辑，不会终止的MR作业。<br>Spout：消息源<br>Bolt：消息处理逻辑。多个Bolt之间有依赖关系，DAG组织。<br>Task：Spout和Bolt可以被并行化拆分为多个处理单元，每个单元为一个Task<br>Stream Grouping：消息分发策略，7种：随机、按字段、广播等。<br>如下图：<br><img src="http://7xir15.com1.z0.glb.clouddn.com/storm组件.PNG" alt="Storm各组件"></p>
<h4 id="3-1_wordcount示例">3.1 wordcount示例</h4><p>还是以wordcount为例，代码在github上：<a href="https://github.com/apache/storm/tree/master/examples/storm-starter" target="_blank" rel="external">点这里</a><br>wordcount分为1个Spout和2个Bolt，流程很简单:<br>RandomSentenceSpout-&gt;SplitSentence-&gt;WordCount</p>
<p><strong>创建TopologyBuilder，设置Spout、bolt，然后提交此拓扑。</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">   TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line">   builder.setSpout(<span class="string">"spout"</span>, <span class="keyword">new</span> RandomSentenceSpout(), <span class="number">5</span>);		<span class="comment">//5为并发消息源任务数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//8为Split并发任务数；shuffleGrouping指定了从Spout到SplitBolt的消息分发策略：随机</span></span><br><span class="line">   builder.setBolt(<span class="string">"split"</span>, <span class="keyword">new</span> SplitSentence(), <span class="number">8</span>).shuffleGrouping(<span class="string">"spout"</span>);</span><br><span class="line">   <span class="comment">//12为计数并发任务书；fieldsGrouping指定了从SplitBolt到WordCount Bolt的消息分发策略：按字段分组，保证同一单词分配到同一task</span></span><br><span class="line">   builder.setBolt(<span class="string">"count"</span>, <span class="keyword">new</span> WordCount(), <span class="number">12</span>).fieldsGrouping(<span class="string">"split"</span>, <span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">   ...</span><br><span class="line">     cluster.submitTopology(<span class="string">"word-count"</span>, conf, builder.createTopology());</span><br></pre></td></tr></table></figure></p>
<p>Spout的作用就是源源不断的产生数据，形象的描述就是一个“水龙头”。<br>示例代码中的Spout在open中先定义了一个随机数生成器，之后Storm框架会不断的调用<code>nextTuple</code>，每次随机从5条字符串中选取一条作为Tuple送到后面的Bolt。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RandomSentenceSpout</span> <span class="keyword">extends</span> <span class="title">BaseRichSpout</span> </span>&#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">    <span class="number">_</span>collector = collector;</span><br><span class="line">    <span class="number">_</span>rand = <span class="keyword">new</span> Random();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Utils.sleep(<span class="number">100</span>);</span><br><span class="line">    String[] sentences = <span class="keyword">new</span> String[]&#123; <span class="string">"the cow jumped over the moon"</span>, <span class="string">"an apple a day keeps the doctor away"</span>,</span><br><span class="line">        <span class="string">"four score and seven years ago"</span>, <span class="string">"snow white and the seven dwarfs"</span>, <span class="string">"i am at two with nature"</span> &#125;;</span><br><span class="line">    String sentence = sentences[<span class="number">_</span>rand.nextInt(sentences.length)];	<span class="comment">//随机抽取一条字符串</span></span><br><span class="line">    <span class="number">_</span>collector.emit(<span class="keyword">new</span> Values(sentence));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>Spolt丢出来的Tuple消息是一个多个单词组成的字符串，SplitBolt会先把它Split为多个单词</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitSentence</span> <span class="keyword">extends</span> <span class="title">ShellBolt</span> <span class="keyword">implements</span> <span class="title">IRichBolt</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SplitSentence</span><span class="params">()</span> </span>&#123;	<span class="comment">//调用python脚本来拆字符串</span></span><br><span class="line">      <span class="keyword">super</span>(<span class="string">"python"</span>, <span class="string">"splitsentence.py"</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>脚本将字符串Split以后再emit（发射）出去给下一个bolt。注意每次发射的是单个单词。<br>此脚本的路径是<code>./multilang/resources/splitsentence.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> storm</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitSentenceBolt</span><span class="params">(storm.BasicBolt)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, tup)</span>:</span></span><br><span class="line">        words = tup.values[<span class="number">0</span>].split(<span class="string">" "</span>)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">          storm.emit([word])</span><br><span class="line"></span><br><span class="line">SplitSentenceBolt().run()</span><br></pre></td></tr></table></figure></p>
<p><strong>WordCount bolt将前面bolt发射出来的单词汇总起来，建立单词与词频的映射关系</strong><br>由于采用了Field Grouping策略，WordCount bolt只要写入Map即可。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span> </span>&#123;</span><br><span class="line">  Map&lt;String, Integer&gt; counts = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</span><br><span class="line">    String word = tuple.getString(<span class="number">0</span>);</span><br><span class="line">    Integer count = counts.get(word);</span><br><span class="line">    <span class="keyword">if</span> (count == <span class="keyword">null</span>)</span><br><span class="line">      count = <span class="number">0</span>;</span><br><span class="line">    count++;</span><br><span class="line">    counts.put(word, count);		<span class="comment">//写入Map表</span></span><br><span class="line">    collector.emit(<span class="keyword">new</span> Values(word, count));	<span class="comment">//继续向后发射</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="4_基本架构">4 基本架构</h3><p>Storm仍然为M/S架构，通过zookeeper通信。主要由下面几个组件构成：</p>
<ul>
<li>控制节点：Nimbus，类似job tracker，分发代码、工作任务</li>
<li>工作节点：Supervisor，类似task tracker，根据需要启动关闭工作进程(Worker)</li>
<li>Worker：负责执行具体任务的逻辑</li>
<li>Task: Worker中每一个Spout/Bolt称为一个Task。0.8以后可以在一个线程中运行多个Spout/Bolt，这个线程称为Executor。<br>下图描述了一个任务的启动过程：<br><img src="http://tech.uc.cn/wp-content/uploads/2013/09/%E6%8F%90%E4%BA%A42.jpg" alt=""></li>
</ul>
<p>Numbus和SupervisorNumbus和Supervisor不直接交互，状态都保存在zookeeper上，故而重启不影响Storm。<br>Worker之间使用MQ传递消息。<br><img src="http://tech.uc.cn/wp-content/uploads/2013/09/%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE.png" alt="nimbus and Supervisor"></p>
<h3 id="5_记录级容错">5 记录级容错</h3><p>所谓记录级，指的是一条tuple被所有应该走到的节点处理完毕。Storm的记录级容错使用了这样一个数学原理：<br>A xor A = 0.<br>A xor B…xor B xor A = 0，其中每一个操作数出现且仅出现两次。<br>Storm要求Spout发射消息时，将tuple id告知acker；要求Bolt发射消息时，将处理的tuple id和新生成的tuple id告知acker。这样所有节点处理完毕后，acker的异或结果为0。<br>如果acker在超时时间内检查不为0，则此记录失败。下面链接中淘宝的文章详细的说明了这一过程，不再赘述。</p>
<p>参考：<br><a href="http://www.searchtb.com/2012/09/introduction-to-storm.html" target="_blank" rel="external">淘宝：storm简介</a><br><a href="http://tech.uc.cn/?p=2159" target="_blank" rel="external">UC：Storm：最火的流式处理框架</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="1_背景：MR的问题">1 背景：MR的问题</h3><ul>
<li>启动时间长。多采用pull模型，没有JVM缓存池</li>
<li>调度开销大</li>
<li>中间数据写磁盘</li>
</ul>
<p>storm的出现，可以比较好的解决上面的问题。<br>]]>
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
</feed>