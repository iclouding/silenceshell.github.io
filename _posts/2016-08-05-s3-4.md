---
layout: post
title: "Spark支持S3作为DataSource（四）：使用Spark处理存储在S3上的图片文件"
date: 2016-08-05 00:11:12
author: 伊布
categories: tech
tags: spark
cover:  "/assets/instacode.png"
---

前面三篇文章介绍了S3以及如何使用Hadoop和Spark处理S3上的文本文件，但毕竟我们使用S3的目的是为了处理非结构化文件（图片，视频）。本文介绍了如何使用Spark处理存储在S3某一个bucket里大量文件（额实际我只放了很少几张图片）的方法。由于我不了解图片处理算法，所以图片的处理只是简单读取了该图片的长度、宽度、拍摄时间、拍摄地点等信息。至于更复杂的图像处理、视频处理，如车牌识别，如果是纯JAVA的实现，那只是更复杂的处理；如果是C++的实现，抱歉不涉及。

实际上编码比较简单，但由于我不太熟悉Spark App，代码写的不太顺利，另外JavaSparkContext.binaryFiles使用的人很少，故此在这里记录。

### 框架代码

```
  private static void wordCount(String[] args) {
    if (args.length < 1) {
        System.err.println("Usage: JavaWordCount <file>");
        System.exit(1);
    }

    SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount");
    JavaSparkContext ctx = new JavaSparkContext(sparkConf);

    ctx.hadoopConfiguration().set("fs.s3a.access.key", "yourAccess");
    ctx.hadoopConfiguration().set("fs.s3a.secret.key", "yourKey");
    ctx.hadoopConfiguration().set("fs.s3a.connection.ssl.enabled", "false");
    ctx.hadoopConfiguration().set("fs.s3a.endpoint", "1.2.3.4:80");

    JavaPairRDD<String, PortableDataStream> myRdd = ctx.binaryFiles(args[0]);
    List <Tuple2<String, String>> output = myRdd.map(new PicProcess()).collect();
    for (Tuple2<?,?> tuple : output) {
        System.out.println(tuple._1() + ": " + tuple._2());
    }

    ctx.stop();
  }
```

1. 使用binaryFiles（显然）。这个方法要求入参是一个路径，而非文件，类似于wholeTextFiles而非textFile。其返回值为JavaPairRDD，RDD的(K,V)

JavaPairRDD.map要求一个function，看上去是一个函数/方法，但实际要求的是org.apache.spark.api.java.function.Function类型的对象。在这里我给的是一个PicProcess类。

map后是

> todo: 还没写完，周末完工

### 框架


### 图片处理
